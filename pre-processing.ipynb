{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48f072a",
   "metadata": {},
   "source": [
    "# Text Representation and Pre-processing Practice\n",
    "\n",
    "This notebook walks through encoding text data into machine-friendly representations\n",
    "using a small sample from the provided tweets dataset.\n",
    "We explore naive one-hot encodings, demonstrate how simple pre-processing helps reduce sparsity,\n",
    "and then build co-occurrence matrices, term-frequency representations, TF–IDF, and compute distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d7888",
   "metadata": {},
   "source": [
    "## Loading a sample of tweets\n",
    "\n",
    "We start by loading the `tweets.json` file and inspecting a small sample.\n",
    "The data is stored as a JSON object with a single key `tweets` mapping to a list of tweet records.\n",
    "For demonstration purposes, we'll use the first two tweets from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "172a912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 tweets from the file.\n",
      "Sample tweet 1: RT @mike_pence: Huge crowd gathered tonight at SNHU Arena in Manchester, NH for @realDonaldTrump! https://t.co/SvnB8xWHKm\n",
      "Sample tweet 2: Springsteen said Hillary was born to run? She can't even walk. @realDonaldTrump @TomiLahren @WeNeedTrump\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the tweets file\n",
    "file_path = 'tweets.json'\n",
    "\n",
    "# Load the data\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the list of tweets\n",
    "tweets = data['tweets']\n",
    "\n",
    "# Take a small sample (two tweets)\n",
    "sample_texts = []\n",
    "for tweet in tweets:\n",
    "    text = tweet.get('full_text') or tweet.get('text') or ''\n",
    "    if text:\n",
    "        sample_texts.append(text)\n",
    "    if len(sample_texts) >= 2:\n",
    "        break\n",
    "\n",
    "print(f\"Loaded {len(tweets)} tweets from the file.\")\n",
    "print('Sample tweet 1:', sample_texts[0])\n",
    "print('Sample tweet 2:', sample_texts[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cad50a",
   "metadata": {},
   "source": [
    "## Naïve one-hot encoding (no pre-processing)\n",
    "\n",
    "To illustrate the limitations of one-hot encodings, we'll first build a one-hot representation without any pre-processing. This means we simply split each tweet on whitespace without removing mentions, URLs, punctuation or changing case.\n",
    "\n",
    "This approach often yields a very large vocabulary containing many one-off tokens (e.g., user handles, URL fragments), which leads to extremely sparse vectors and little overlap between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e96b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive tokens:\n",
      "Tweet 1 tokens: ['RT', '@mike_pence:', 'Huge', 'crowd', 'gathered', 'tonight', 'at', 'SNHU', 'Arena', 'in', 'Manchester,', 'NH', 'for', '@realDonaldTrump!', 'https://t.co/SvnB8xWHKm']\n",
      "Tweet 2 tokens: ['Springsteen', 'said', 'Hillary', 'was', 'born', 'to', 'run?', 'She', \"can't\", 'even', 'walk.', '@realDonaldTrump', '@TomiLahren', '@WeNeedTrump']\n",
      "Vocabulary size (naive): 29\n",
      "Vocabulary: ['@TomiLahren', '@WeNeedTrump', '@mike_pence:', '@realDonaldTrump', '@realDonaldTrump!', 'Arena', 'Hillary', 'Huge', 'Manchester,', 'NH', 'RT', 'SNHU', 'She', 'Springsteen', 'at', 'born', \"can't\", 'crowd', 'even', 'for', 'gathered', 'https://t.co/SvnB8xWHKm', 'in', 'run?', 'said', 'to', 'tonight', 'walk.', 'was']\n",
      "One-hot vector for tweet 1: [0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0]\n",
      "One-hot vector for tweet 2: [1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1]\n",
      "Overlap (dot product) without pre-processing: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Naive tokenization by splitting on whitespace (no cleaning)\n",
    "naive_tokens = [text.split() for text in sample_texts]\n",
    "print(\"Naive tokens:\")\n",
    "for i, toks in enumerate(naive_tokens, 1):\n",
    "    print(f\"Tweet {i} tokens: {toks}\")\n",
    "\n",
    "# Build vocabulary from naive tokens\n",
    "vocab_naive = sorted(set().union(*naive_tokens))\n",
    "word2idx_naive = {w: i for i, w in enumerate(vocab_naive)}\n",
    "\n",
    "# Create one-hot presence vectors\n",
    "vectors_naive = []\n",
    "for toks in naive_tokens:\n",
    "    s = set(toks)\n",
    "    vec = [1 if w in s else 0 for w in vocab_naive]\n",
    "    vectors_naive.append(vec)\n",
    "\n",
    "print('Vocabulary size (naive):', len(vocab_naive))\n",
    "print('Vocabulary:', vocab_naive)\n",
    "\n",
    "# Display one-hot vectors and overlap\n",
    "for i, vec in enumerate(vectors_naive, 1):\n",
    "    print(f\"One-hot vector for tweet {i}: {vec}\")\n",
    "\n",
    "# Compute overlap (dot product) between the two vectors\n",
    "if len(vectors_naive) >= 2:\n",
    "    from numpy import dot\n",
    "    overlap_naive = dot(vectors_naive[0], vectors_naive[1])\n",
    "    print('Overlap (dot product) without pre-processing:', overlap_naive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9262d646",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Without any pre-processing, the vocabulary contains many tokens that are unique to one tweet (e.g., mentions, URL fragments, mixed case words). As a result, the one-hot vectors share very few common dimensions and the overlap (dot product) is tiny. This highlights the limitations of naive one-hot encoding. Next, we clean and normalize the text to create more meaningful representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d646d",
   "metadata": {},
   "source": [
    "## Pre-processing: Tokenization and normalization\n",
    "\n",
    "A simple preprocessing pipeline helps reduce sparsity by lowercasing, removing punctuation, and splitting tweets into consistent tokens. This step is crucial for aggregating similar words and improving overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295f10e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens after preprocessing:\n",
      "Tweet 1 tokens: ['rt', 'mike', 'pence', 'huge', 'crowd', 'gathered', 'tonight', 'at', 'snhu', 'arena', 'in', 'manchester', 'nh', 'for', 'realdonaldtrump', 'https', 't', 'co', 'svnb8xwhkm']\n",
      "Tweet 2 tokens: ['springsteen', 'said', 'hillary', 'was', 'born', 'to', 'run', 'she', 'can', 't', 'even', 'walk', 'realdonaldtrump', 'tomilahren', 'weneedtrump']\n",
      "Vocabulary size (cleaned): 32\n",
      "Vocabulary: ['arena', 'at', 'born', 'can', 'co', 'crowd', 'even', 'for', 'gathered', 'hillary', 'https', 'huge', 'in', 'manchester', 'mike', 'nh', 'pence', 'realdonaldtrump', 'rt', 'run', 'said', 'she', 'snhu', 'springsteen', 'svnb8xwhkm', 't', 'to', 'tomilahren', 'tonight', 'walk', 'was', 'weneedtrump']\n",
      "One-hot vector for tweet 1: [1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "One-hot vector for tweet 2: [0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n",
      "Overlap (dot product) after preprocessing: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# A simple tokenizer: lowercase and remove non-alphanumeric characters\n",
    "\n",
    "def tokenize(text):\n",
    "    cleaned = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text.lower())\n",
    "    return cleaned.split()\n",
    "\n",
    "# Apply tokenizer to each sample tweet\n",
    "tokenized = [tokenize(t) for t in sample_texts]\n",
    "print(\"Tokens after preprocessing:\")\n",
    "for i, toks in enumerate(tokenized, 1):\n",
    "    print(f\"Tweet {i} tokens: {toks}\")\n",
    "\n",
    "# Build vocabulary from the cleaned tokens\n",
    "vocab = sorted(set().union(*tokenized))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# Create one-hot presence vectors\n",
    "vectors = []\n",
    "for toks in tokenized:\n",
    "    s = set(toks)\n",
    "    vec = [1 if w in s else 0 for w in vocab]\n",
    "    vectors.append(vec)\n",
    "\n",
    "print('Vocabulary size (cleaned):', len(vocab))\n",
    "print('Vocabulary:', vocab)\n",
    "\n",
    "# Display one-hot vectors and overlap\n",
    "for i, vec in enumerate(vectors, 1):\n",
    "    print(f\"One-hot vector for tweet {i}: {vec}\")\n",
    "\n",
    "# Compute overlap (dot product) between the two vectors\n",
    "if len(vectors) >= 2:\n",
    "    from numpy import dot\n",
    "    overlap = dot(vectors[0], vectors[1])\n",
    "    print('Overlap (dot product) after preprocessing:', overlap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ef62b",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "After normalization, many of the one-off tokens (handles, URL fragments) disappear or are converted into consistent lowercased terms. This increases the vocabulary overlap between tweets, though it may still be small with a very small sample. The next sections build richer representations that capture context and frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c910f8a0",
   "metadata": {},
   "source": [
    "## Word × word co-occurrence matrix\n",
    "\n",
    "Co-occurrence counts capture how often words appear near each other within a sliding window. We define a window size \\(k\\) and slide across each tweet, counting word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71258cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence matrix (rows and columns in vocab order):\n",
      "[[0 1 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Window size (feel free to experiment)\n",
    "k = 2\n",
    "\n",
    "# Initialize co-occurrence matrix\n",
    "cooc = np.zeros((len(vocab), len(vocab)), dtype=int)\n",
    "\n",
    "# Populate co-occurrence counts\n",
    "for tokens in tokenized:\n",
    "    for i, w in enumerate(tokens):\n",
    "        if w not in word2idx:\n",
    "            continue\n",
    "        wi = word2idx[w]\n",
    "        # look at k neighbors on each side\n",
    "        window = tokens[max(0, i - k): i] + tokens[i + 1: i + k + 1]\n",
    "        for u in window:\n",
    "            if u in word2idx and u != w:\n",
    "                ui = word2idx[u]\n",
    "                cooc[wi, ui] += 1\n",
    "\n",
    "print(\"Co-occurrence matrix (rows and columns in vocab order):\")\n",
    "print(cooc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9773c",
   "metadata": {},
   "source": [
    "## Word × document (term-frequency) matrix\n",
    "\n",
    "A term-frequency (TF) matrix records how many times each word appears in each document. This is the foundation for bag-of-words and TF–IDF representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd78be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-frequency matrix:\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Number of documents\n",
    "num_docs = len(tokenized)\n",
    "\n",
    "# Term-frequency matrix: rows = words, columns = tweets\n",
    "import numpy as np\n",
    "tf = np.zeros((len(vocab), num_docs), dtype=int)\n",
    "\n",
    "for j, tokens_list in enumerate(tokenized):\n",
    "    counts = Counter(tokens_list)\n",
    "    for i, w in enumerate(vocab):\n",
    "        tf[i, j] = counts[w]\n",
    "\n",
    "print('Term-frequency matrix:')\n",
    "print(tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f259c9",
   "metadata": {},
   "source": [
    "### Deriving co-occurrence from the TF matrix\n",
    "\n",
    "Multiplying the TF matrix by its transpose (\\(TF \times TF^\top\\)) gives a simple co-occurrence count where counts are aggregated at the document level. (This ignores sliding windows and instead counts words co-occurring within the same document.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4316f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence derived from TF (TF @ TF.T):\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 1 1 1]\n",
      " ...\n",
      " [0 0 1 ... 1 1 1]\n",
      " [0 0 1 ... 1 1 1]\n",
      " [0 0 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute a document-level co-occurrence estimate\n",
    "cooc_from_tf = tf @ tf.T\n",
    "print('Co-occurrence derived from TF (TF @ TF.T):')\n",
    "print(cooc_from_tf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052468e2",
   "metadata": {},
   "source": [
    "## TF–IDF weighting\n",
    "\n",
    "Term frequency–inverse document frequency (TF–IDF) downweights very common words and upweights terms that are rare across documents. It is widely used in information retrieval and text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c559c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF vector:\n",
      "[1.405 1.405 1.405 1.405 1.405 1.405 1.405 1.405 1.405 1.405 1.405 1.405\n",
      " 1.405 1.405 1.405 1.405 1.405 1.    1.405 1.405 1.405 1.405 1.405 1.405\n",
      " 1.405 1.    1.405 1.405 1.405 1.405 1.405 1.405]\n",
      "TF–IDF matrix:\n",
      "[[1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [0.    1.405]\n",
      " [0.    1.405]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [0.    1.405]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [0.    1.405]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [1.405 0.   ]\n",
      " [1.    1.   ]\n",
      " [1.405 0.   ]\n",
      " [0.    1.405]\n",
      " [0.    1.405]\n",
      " [0.    1.405]\n",
      " [1.405 0.   ]\n",
      " [0.    1.405]\n",
      " [1.405 0.   ]\n",
      " [1.    1.   ]\n",
      " [0.    1.405]\n",
      " [0.    1.405]\n",
      " [1.405 0.   ]\n",
      " [0.    1.405]\n",
      " [0.    1.405]\n",
      " [0.    1.405]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compute document frequency (df): number of documents where each word appears\n",
    "df = (tf > 0).sum(axis=1)\n",
    "\n",
    "# Number of documents\n",
    "N = num_docs\n",
    "\n",
    "# Inverse document frequency (smoothed)\n",
    "idf = np.log((N + 1) / (df + 1)) + 1\n",
    "\n",
    "# Compute TF–IDF matrix\n",
    "# tf is integer counts; broadcasting idf across columns\n",
    "tfidf = tf * idf[:, None]\n",
    "\n",
    "print('IDF vector:')\n",
    "print(np.round(idf, 3))\n",
    "print('TF–IDF matrix:')\n",
    "print(np.round(tfidf, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0b63e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we explored how to represent text data for analysis. We started with naive one-hot encodings and observed how their sparsity and lack of preprocessing lead to very little overlap between documents. After tokenization and normalization, we built vocabularies, co-occurrence matrices, bag-of-words. These techniques lay the groundwork for more advanced models such as word embeddings and topic models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
