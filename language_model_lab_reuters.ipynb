{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48ff267",
   "metadata": {},
   "source": [
    "# Language Modeling Lab: From N-grams to Neural LMs\n",
    "\n",
    "This notebook mirrors the lecture progression for the **Language Models** part of the course.\n",
    "\n",
    "It is structured around:\n",
    "\n",
    "1. **Sparsity → preprocessing → n-grams → smoothing**\n",
    "2. **Neural window models → embeddings → better smoothing**\n",
    "3. **Reflections** that connect these models to ideas about RNNs and attention (without implementing them).\n",
    "\n",
    "We will work with:\n",
    "\n",
    "- The **Reuters** newswire corpus (from NLTK)\n",
    "- **MLE unigram and bigram** language models\n",
    "- **UNK smoothing** and **interpolated n-grams**\n",
    "- A **TF–IDF pseudo-LM** (bag-of-words view as a language model)\n",
    "- A **neural language model** with **pretrained word embeddings**\n",
    "- **Perplexity** as a common evaluation metric\n",
    "- Simple **text generation** from the models\n",
    "- Short **reflection prompts** tying back to the lecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b2ce0",
   "metadata": {},
   "source": [
    "## 1. Setup and Reuters Corpus\n",
    "\n",
    "We start by:\n",
    "\n",
    "- Importing libraries\n",
    "- Downloading and loading the Reuters corpus from NLTK\n",
    "- Creating a train / validation split\n",
    "- Building a tokenization and preprocessing pipeline\n",
    "- Introducing `<UNK>` to handle rare words and mitigate sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1359a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/julio/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/julio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 163625 sentences from Reuters.\n",
      "Train sentences: 147262, validation sentences: 16363\n",
      "Vocab size (including special tokens): 17645\n",
      "Example preprocessed sentence:\n",
      "['<BOS>', 'the', 'peak', 'broke', 'a', 'steady', 'trading', 'trend', 'of', '15', 'to', '17', 'billion', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import math  # For logarithm calculations in perplexity\n",
    "import random  # For shuffling data and generating text\n",
    "from collections import Counter, defaultdict  # For counting words efficiently\n",
    "\n",
    "import nltk  # Natural Language Toolkit\n",
    "from nltk.corpus import reuters  # Reuters news corpus\n",
    "from nltk import word_tokenize  # Splits text into words\n",
    "\n",
    "# Download required NLTK data (only needed the first time)\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Set random seed for reproducibility (same results every time)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def get_reuters_sentences(min_len=3, max_len=40):\n",
    "    \"\"\"Return tokenized sentences from the Reuters corpus.\n",
    "    \n",
    "    We filter sentences by length to focus on typical newswire sentences\n",
    "    and avoid very short fragments or extremely long paragraphs.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for file_id in reuters.fileids():\n",
    "        raw = reuters.raw(file_id)\n",
    "        for line in raw.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            tokens = word_tokenize(line)\n",
    "            # Keep only sentences within the desired length range\n",
    "            if min_len <= len(tokens) <= max_len:\n",
    "                sentences.append(tokens)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Load all sentences from the Reuters corpus\n",
    "sentences = get_reuters_sentences()\n",
    "print(f\"Collected {len(sentences)} sentences from Reuters.\")\n",
    "\n",
    "\n",
    "# Split data into training and validation sets (90% train, 10% validation)\n",
    "random.shuffle(sentences)  # Randomize order to avoid any systematic bias\n",
    "split_idx = int(0.9 * len(sentences))  # Calculate the 90% cutoff point\n",
    "train_sents = sentences[:split_idx]  # First 90% for training\n",
    "valid_sents = sentences[split_idx:]  # Last 10% for validation\n",
    "print(f\"Train sentences: {len(train_sents)}, validation sentences: {len(valid_sents)}\")\n",
    "\n",
    "\n",
    "def build_vocab(train_sents, min_freq=3):\n",
    "    \"\"\"Build vocabulary from training sentences, applying an UNK threshold.\n",
    "    \n",
    "    Words appearing fewer than min_freq times are excluded from the vocabulary\n",
    "    and will be replaced with <UNK> during preprocessing.\n",
    "    \"\"\"\n",
    "    # Counter is a special dictionary that counts how many times each word appears\n",
    "    freq = Counter()\n",
    "    \n",
    "    # Loop through all training sentences\n",
    "    for sent in train_sents:\n",
    "        # Count each word (after converting to lowercase)\n",
    "        freq.update(token.lower() for token in sent)\n",
    "    \n",
    "    # Start with special tokens (used for sentence boundaries and rare words)\n",
    "    vocab = {\"<UNK>\", \"<BOS>\", \"<EOS>\"}\n",
    "    \n",
    "    # Add words that appear frequently enough\n",
    "    for w, c in freq.items():  # w = word, c = count\n",
    "        if c >= min_freq:  # Only keep words that appear at least min_freq times\n",
    "            vocab.add(w)\n",
    "    \n",
    "    # Return both the vocabulary set and the frequency counts\n",
    "    return vocab, freq\n",
    "\n",
    "\n",
    "# Build vocabulary from training data\n",
    "vocab, freq = build_vocab(train_sents)\n",
    "print(f\"Vocab size (including special tokens): {len(vocab)}\")\n",
    "\n",
    "\n",
    "def preprocess_sentence(tokens, vocab):\n",
    "    \"\"\"Lowercase, map rare tokens to <UNK>, add sentence boundary tokens.\"\"\"\n",
    "    # Start with beginning-of-sentence marker\n",
    "    processed = [\"<BOS>\"]\n",
    "    \n",
    "    # Process each token in the sentence\n",
    "    for tok in tokens:\n",
    "        tok = tok.lower()  # Convert to lowercase\n",
    "        \n",
    "        # If word is not in our vocabulary, replace with <UNK>\n",
    "        if tok not in vocab:\n",
    "            tok = \"<UNK>\"\n",
    "        \n",
    "        processed.append(tok)\n",
    "    \n",
    "    # End with end-of-sentence marker\n",
    "    processed.append(\"<EOS>\")\n",
    "    return processed\n",
    "\n",
    "\n",
    "# Apply preprocessing to all training and validation sentences\n",
    "# List comprehension: [function(item) for item in list] creates a new list\n",
    "train_proc = [preprocess_sentence(s, vocab) for s in train_sents]\n",
    "valid_proc = [preprocess_sentence(s, vocab) for s in valid_sents]\n",
    "\n",
    "# Show an example of what a preprocessed sentence looks like\n",
    "print(\"Example preprocessed sentence:\")\n",
    "print(train_proc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f630de",
   "metadata": {},
   "source": [
    "### Understanding the Preprocessing Pipeline\n",
    "\n",
    "The code above implements several key preprocessing steps to handle **data sparsity**:\n",
    "\n",
    "1. **Vocabulary building**: We only keep words that appear at least `min_freq` times in training data\n",
    "2. **Special tokens**:\n",
    "   - `<UNK>`: Replaces rare words (those not in our vocabulary)\n",
    "   - `<BOS>` and `<EOS>`: Mark sentence boundaries, helping the model learn beginnings and endings\n",
    "3. **Lowercasing**: Reduces sparsity by treating \"The\" and \"the\" as the same word\n",
    "\n",
    "**Why is this important?** Without these steps, our language model would see thousands of words that appear only once or twice, making it impossible to learn meaningful patterns from limited data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95fc1c",
   "metadata": {},
   "source": [
    "### What is Maximum Likelihood Estimation (MLE)?\n",
    "\n",
    "MLE language models estimate probabilities by simply **counting occurrences** in the training data:\n",
    "\n",
    "- **Unigram**: $P(w) = \\frac{\\text{count}(w)}{\\text{total words}}$\n",
    "- **Bigram**: $P(w_t \\mid w_{t-1}) = \\frac{\\text{count}(w_{t-1}, w_t)}{\\text{count}(w_{t-1})}$\n",
    "\n",
    "This is the \"maximum likelihood\" approach because it assigns the highest probability to the patterns we observed most frequently. It's simple and intuitive, but has a major flaw: if we never saw a word pair in training, the probability is zero!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e82db",
   "metadata": {},
   "source": [
    "## 2. MLE Unigram and Bigram Language Models\n",
    "\n",
    "We now build **maximum-likelihood** unigram and bigram LMs:\n",
    "\n",
    "- Count how often each word and word pair appears in the training data\n",
    "- Convert counts into conditional probabilities\n",
    "- Use these probabilities to compute sentence likelihoods\n",
    "- Evaluate via **perplexity**\n",
    "- Sample simple continuations from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef5b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity (train): 461.532203786521\n",
      "Bigram perplexity (train): 58.73253378585444\n",
      "\n",
      "Generated sentences (unigram):\n",
      "  Example 1: <BOS> a the <UNK> <BOS> . the resources billion inc writedowns and a the essentially <UNK> by <UNK> <EOS>\n",
      "  Example 2: <BOS> . public special with for reported six could & <EOS>\n",
      "\n",
      "Generated sentences (bigram):\n",
      "  Example 1: <BOS> much producers . <EOS>\n",
      "  Example 2: <BOS> fernandez said . <EOS>\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class UnigramLM:\n",
    "    \"\"\"Unigram Language Model: predicts words based only on their overall frequency.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize data structures to store word statistics\n",
    "        self.counts = Counter[Any]()  # Dictionary storing how many times each word appears\n",
    "        self.total = 0  # Total number of words seen\n",
    "        self.vocab = set[Any]()  # Set of all unique words\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"Train the model by counting words in the training sentences.\"\"\"\n",
    "        # Loop through each sentence\n",
    "        for sent in sentences:\n",
    "            # Loop through each word in the sentence\n",
    "            for w in sent:\n",
    "                self.counts[w] += 1  # Increment count for this word\n",
    "                self.total += 1  # Increment total word count\n",
    "                self.vocab.add(w)  # Add to vocabulary set\n",
    "\n",
    "    def prob(self, w):\n",
    "        \"\"\"Calculate probability of a word: P(w) = count(w) / total_words\"\"\"\n",
    "        # Divide word count by total to get probability\n",
    "        return self.counts[w] / self.total if self.total > 0 else 0.0\n",
    "\n",
    "    def sent_log_prob(self, sent):\n",
    "        \"\"\"Calculate log probability of a sentence: sum of log probabilities of each word.\n",
    "        \n",
    "        We use logs because multiplying many small probabilities can cause numerical issues.\n",
    "        Instead of P(w1) * P(w2) * P(w3), we compute log(P(w1)) + log(P(w2)) + log(P(w3))\n",
    "        \"\"\"\n",
    "        log_p = 0.0  # Initialize sum to zero\n",
    "        \n",
    "        # Calculate log probability for each word\n",
    "        for w in sent:\n",
    "            p = self.prob(w)  # Get probability of this word\n",
    "            if p == 0.0:\n",
    "                return float(\"-inf\")  # log(0) is undefined, return negative infinity\n",
    "            log_p += math.log(p)  # Add log of probability\n",
    "        \n",
    "        return log_p\n",
    "\n",
    "    def generate(self, max_len=20):\n",
    "        \"\"\"Generate a random sentence by sampling words according to their probabilities.\"\"\"\n",
    "        words = [\"<BOS>\"]  # Start with beginning-of-sentence token\n",
    "        \n",
    "        # Generate up to max_len words\n",
    "        for _ in range(max_len):\n",
    "            # Sample a word proportional to its frequency\n",
    "            word_list = list(self.counts.keys())  # Get all words we've seen\n",
    "            weights = [self.counts[w] for w in word_list]  # Get their counts as weights\n",
    "            next_word = random.choices(word_list, weights=weights, k=1)[0]  # Pick one word\n",
    "            words.append(next_word)\n",
    "            \n",
    "            # Stop if we generate the end-of-sentence token\n",
    "            if next_word == \"<EOS>\":\n",
    "                break\n",
    "        \n",
    "        return words\n",
    "\n",
    "\n",
    "class BigramLM:\n",
    "    \"\"\"Bigram Language Model: predicts next word based on the previous word.\n",
    "    \n",
    "    P(w_next | w_prev) = count(w_prev, w_next) / count(w_prev)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab):\n",
    "        # defaultdict(Counter) creates a dictionary where each value is automatically a Counter\n",
    "        # This lets us store bigram_counts[word1][word2] = count\n",
    "        self.bigram_counts = defaultdict(Counter)  # Counts for word pairs\n",
    "        self.unigram_counts = Counter()  # Counts for individual words\n",
    "        self.vocab = vocab  # Vocabulary set\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"Train the model by counting word pairs (bigrams) in training sentences.\"\"\"\n",
    "        # Loop through each sentence\n",
    "        for sent in sentences:\n",
    "            # Loop through consecutive word pairs (bigrams)\n",
    "            for i in range(len(sent) - 1):\n",
    "                w = sent[i]  # Current word\n",
    "                w_next = sent[i + 1]  # Next word\n",
    "                self.unigram_counts[w] += 1  # Count current word\n",
    "                self.bigram_counts[w][w_next] += 1  # Count the word pair\n",
    "            \n",
    "            # Don't forget to count the last word (it has no successor)\n",
    "            self.unigram_counts[sent[-1]] += 1\n",
    "\n",
    "    def prob(self, w_next, w):\n",
    "        \"\"\"Calculate P(w_next | w) = count(w, w_next) / count(w)\"\"\"\n",
    "        denom = self.unigram_counts[w]  # How many times did we see w?\n",
    "        if denom == 0:\n",
    "            return 0.0  # Never saw this word, so probability is 0\n",
    "        # Divide bigram count by unigram count to get conditional probability\n",
    "        return self.bigram_counts[w][w_next] / denom\n",
    "\n",
    "    def sent_log_prob(self, sent):\n",
    "        \"\"\"Calculate log probability of a sentence using bigram probabilities.\"\"\"\n",
    "        log_p = 0.0  # Initialize sum\n",
    "        \n",
    "        # Loop through consecutive word pairs\n",
    "        for i in range(len(sent) - 1):\n",
    "            w = sent[i]  # Current word\n",
    "            w_next = sent[i + 1]  # Next word\n",
    "            p = self.prob(w_next, w)  # Get P(next | current)\n",
    "            if p == 0.0:\n",
    "                return float(\"-inf\")  # Unseen bigram\n",
    "            log_p += math.log(p)  # Add log probability\n",
    "        \n",
    "        return log_p\n",
    "\n",
    "    def generate(self, max_len=20):\n",
    "        \"\"\"Generate text by predicting each word based on the previous word.\"\"\"\n",
    "        cur = \"<BOS>\"  # Start with beginning-of-sentence token\n",
    "        words = [cur]  # List to store generated words\n",
    "        \n",
    "        # Generate up to max_len words\n",
    "        for _ in range(max_len):\n",
    "            # Get all words that can follow the current word\n",
    "            next_candidates = self.bigram_counts[cur]\n",
    "            if not next_candidates:\n",
    "                break  # No continuations found, stop generating\n",
    "            \n",
    "            # Sample next word based on bigram probabilities\n",
    "            word_list = list(next_candidates.keys())  # Possible next words\n",
    "            weights = [next_candidates[w] for w in word_list]  # Their counts\n",
    "            cur = random.choices(word_list, weights=weights, k=1)[0]  # Pick one\n",
    "            words.append(cur)\n",
    "            \n",
    "            # Stop if we generate end-of-sentence\n",
    "            if cur == \"<EOS>\":\n",
    "                break\n",
    "        \n",
    "        return words\n",
    "\n",
    "\n",
    "def perplexity(model, sentences, use_bigrams=False):\n",
    "    \"\"\"Calculate perplexity: a measure of how well the model predicts the data.\n",
    "    \n",
    "    Lower perplexity = better model. Perplexity is roughly \"how many words the model \n",
    "    is confused between at each step\". For example, perplexity of 100 means the model\n",
    "    is as uncertain as if it had to choose uniformly among 100 words.\n",
    "    \"\"\"\n",
    "    total_log_prob = 0.0  # Sum of log probabilities across all sentences\n",
    "    total_tokens = 0  # Total number of words/tokens\n",
    "    \n",
    "    # Calculate log probability for each sentence\n",
    "    for sent in sentences:\n",
    "        log_p = model.sent_log_prob(sent)  # Get sentence log probability\n",
    "        \n",
    "        # Count tokens differently for unigrams vs bigrams\n",
    "        if use_bigrams:\n",
    "            # For bigrams, we predict len(sent)-1 words (each word after the first)\n",
    "            n_tokens = max(len(sent) - 1, 1)\n",
    "        else:\n",
    "            # For unigrams, we predict all words\n",
    "            n_tokens = len(sent)\n",
    "        \n",
    "        total_log_prob += log_p\n",
    "        total_tokens += n_tokens\n",
    "    \n",
    "    # Calculate average log probability per token\n",
    "    avg_log_p = total_log_prob / total_tokens\n",
    "    \n",
    "    # Convert to perplexity: exp(-average_log_prob)\n",
    "    return math.exp(-avg_log_p)\n",
    "\n",
    "\n",
    "# Create and train a unigram model\n",
    "uni_lm = UnigramLM()\n",
    "uni_lm.fit(train_proc)\n",
    "\n",
    "# Create and train a bigram model\n",
    "bi_lm = BigramLM(vocab=vocab)\n",
    "bi_lm.fit(train_proc)\n",
    "\n",
    "# Evaluate both models on training data\n",
    "print(\"Unigram perplexity (train):\", perplexity(uni_lm, train_proc))\n",
    "print(\"Bigram perplexity (train):\", perplexity(bi_lm, train_proc, use_bigrams=True))\n",
    "\n",
    "# Generate examples from unigram model\n",
    "# Note: Unigram output looks very random because it only considers word frequency,\n",
    "# ignoring all context and word order\n",
    "print(\"\\nGenerated sentences (unigram):\")\n",
    "print(\"  Example 1:\", \" \".join(uni_lm.generate()))\n",
    "print(\"  Example 2:\", \" \".join(uni_lm.generate()))\n",
    "\n",
    "# Generate examples from bigram model\n",
    "# Note: Bigram output shows local coherence (reasonable word pairs) but may lack\n",
    "# long-range structure and can get stuck in loops or dead-ends\n",
    "print(\"\\nGenerated sentences (bigram):\")\n",
    "print(\"  Example 1:\", \" \".join(bi_lm.generate()))\n",
    "print(\"  Example 2:\", \" \".join(bi_lm.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855e39f4",
   "metadata": {},
   "source": [
    "## 3. Smoothing and Interpolated N-grams\n",
    "\n",
    "Even with `<UNK>`, many bigrams will be unseen in training. To reduce zero probabilities we:\n",
    "\n",
    "1. Keep the MLE unigram and bigram counts.\n",
    "2. Define an **interpolated model**:\n",
    "\n",
    "\\begin{align}\n",
    "P(w_{t} \\mid w_{t-1}) = \\lambda_{1} P_{\\text{unigram}}(w_{t}) + \\lambda_{2} P_{\\text{bigram}}(w_{t} \\mid w_{t-1})\n",
    "\\end{align}\n",
    "\n",
    "3. Optionally add a small add-k (Laplace) smoothing term on bigrams to avoid zero probabilities entirely.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Implement an interpolated LM\n",
    "- Explore how interpolation constants affect perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c57a90",
   "metadata": {},
   "source": [
    "### Understanding Interpolation and Add-k Smoothing\n",
    "\n",
    "**Interpolation** combines models of different orders to balance specificity with generalization:\n",
    "\n",
    "$$P(w_{t} \\mid w_{t-1}) = \\lambda_{1} P_{\\text{unigram}}(w_{t}) + \\lambda_{2} P_{\\text{bigram}}(w_{t} \\mid w_{t-1})$$\n",
    "\n",
    "where $\\lambda_1 + \\lambda_2 = 1$. If we never saw the bigram $(w_{t-1}, w_t)$, we can still fall back on the unigram probability.\n",
    "\n",
    "**Add-k smoothing** (a form of Laplace smoothing) adds a small constant $k$ to all counts:\n",
    "\n",
    "$$P_{\\text{smooth}}(w_t \\mid w_{t-1}) = \\frac{\\text{count}(w_{t-1}, w_t) + k}{\\text{count}(w_{t-1}) + k \\cdot |V|}$$\n",
    "\n",
    "This ensures no probability is exactly zero, even for unseen bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c0a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolated bigram perplexity (validation): 231.8023664853118\n",
      "\n",
      "Generated sentences (interpolated):\n",
      "  Example 1: <BOS> sheldahl 71.7 veto approve purchases , <EOS>\n",
      "  Example 2: <BOS> in 36 stevens modified preceding advertisement two sub-saharan depleted tractor <BOS> proceeds and is towards <BOS> would pacific telesis one-week\n"
     ]
    }
   ],
   "source": [
    "class InterpolatedBigramLM:\n",
    "    \"\"\"Interpolated Bigram Language Model: combines unigram and bigram probabilities.\n",
    "    \n",
    "    This helps handle unseen word pairs by falling back to unigram probabilities.\n",
    "    P_interpolated = λ₁ * P_unigram + λ₂ * P_bigram\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, unigram_lm, bigram_lm, lambda1=0.3, lambda2=0.7, add_k=1.0):\n",
    "        # Check that lambda weights sum to 1 (they define a probability distribution)\n",
    "        assert abs(lambda1 + lambda2 - 1.0) < 1e-6, \"lambda1 + lambda2 must equal 1.0\"\n",
    "        \n",
    "        # Store the two models we're interpolating\n",
    "        self.uni = unigram_lm  # Unigram model\n",
    "        self.bi = bigram_lm  # Bigram model\n",
    "        \n",
    "        # Store interpolation weights\n",
    "        self.lambda1 = lambda1  # Weight for unigram (typically smaller)\n",
    "        self.lambda2 = lambda2  # Weight for bigram (typically larger)\n",
    "        \n",
    "        self.add_k = add_k  # Smoothing constant\n",
    "        self.vocab = list(unigram_lm.vocab)  # Vocabulary as a list\n",
    "\n",
    "    def prob(self, w_next, w_prev):\n",
    "        \"\"\"Calculate interpolated probability: mix unigram and bigram probabilities.\"\"\"\n",
    "        \n",
    "        # Get unigram probability: P(w_next)\n",
    "        p_uni = self.uni.prob(w_next)\n",
    "\n",
    "        # Get bigram probability with add-k smoothing: P(w_next | w_prev)\n",
    "        bigram_counts = self.bi.bigram_counts[w_prev]  # All words that followed w_prev\n",
    "        count = bigram_counts[w_next]  # How many times w_next followed w_prev\n",
    "        total = self.bi.unigram_counts[w_prev]  # How many times w_prev appeared\n",
    "        V = len(self.uni.vocab)  # Vocabulary size\n",
    "        \n",
    "        # Apply add-k smoothing formula\n",
    "        if total > 0:\n",
    "            p_bi = (count + self.add_k) / (total + self.add_k * V)\n",
    "        else:\n",
    "            p_bi = 1.0 / V  # Uniform distribution if we never saw w_prev\n",
    "\n",
    "        # Combine the two probabilities using the lambda weights\n",
    "        return self.lambda1 * p_uni + self.lambda2 * p_bi\n",
    "\n",
    "    def sent_log_prob(self, sent):\n",
    "        \"\"\"Calculate log probability of a sentence using interpolated probabilities.\"\"\"\n",
    "        log_p = 0.0  # Initialize sum\n",
    "        \n",
    "        # Loop through consecutive word pairs\n",
    "        for i in range(len(sent) - 1):\n",
    "            w = sent[i]  # Current word\n",
    "            w_next = sent[i + 1]  # Next word\n",
    "            p = self.prob(w_next, w)  # Get interpolated probability\n",
    "            log_p += math.log(p)  # Add log of probability\n",
    "        \n",
    "        return log_p\n",
    "\n",
    "    def generate(self, max_len=20):\n",
    "        \"\"\"Generate text using the interpolated model.\"\"\"\n",
    "        cur = \"<BOS>\"  # Start with beginning-of-sentence\n",
    "        words = [cur]  # List to store generated words\n",
    "        \n",
    "        # Generate up to max_len words\n",
    "        for _ in range(max_len):\n",
    "            # Compute interpolated probability for each possible next word\n",
    "            # This considers the entire vocabulary\n",
    "            probs = [self.prob(w_next, cur) for w_next in self.vocab]\n",
    "            \n",
    "            # Sample from the distribution using these probabilities\n",
    "            cur = random.choices(self.vocab, weights=probs, k=1)[0]\n",
    "            words.append(cur)\n",
    "            \n",
    "            # Stop if we generate end-of-sentence\n",
    "            if cur == \"<EOS>\":\n",
    "                break\n",
    "        \n",
    "        return words\n",
    "\n",
    "\n",
    "# Create an interpolated model combining the unigram and bigram models\n",
    "# lambda1=0.3 means 30% weight on unigram, lambda2=0.7 means 70% on bigram\n",
    "interp_lm = InterpolatedBigramLM(uni_lm, bi_lm, lambda1=0.3, lambda2=0.7, add_k=0.1)\n",
    "\n",
    "# Evaluate on validation set (data the model hasn't seen during training)\n",
    "print(\"Interpolated bigram perplexity (validation):\", perplexity(interp_lm, valid_proc, use_bigrams=True))\n",
    "\n",
    "# Generate example sentences\n",
    "# Note: Interpolated model produces smoother output than pure bigram by falling back\n",
    "# to unigram probabilities, reducing the impact of unseen bigrams\n",
    "print(\"\\nGenerated sentences (interpolated):\")\n",
    "print(\"  Example 1:\", \" \".join(interp_lm.generate()))\n",
    "print(\"  Example 2:\", \" \".join(interp_lm.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049fafe",
   "metadata": {},
   "source": [
    "### Reflection 1: Sparsity, Preprocessing, and Smoothing\n",
    "\n",
    "Discuss (in your own words, in the notebook):\n",
    "\n",
    "1. How did preprocessing (lowercasing, `<UNK>`, `<BOS>`, `<EOS>`) change the effective sparsity of the data?\n",
    "2. Compare perplexity for the unigram, bigram, and interpolated models. What do you observe?\n",
    "3. How does this relate to the lecture narrative about:\n",
    "   - One-hot sparsity\n",
    "   - Co-occurrence counts\n",
    "   - The need for better smoothing and generalization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39927d17",
   "metadata": {},
   "source": [
    "## 4. TF-IDF as a Pseudo Language Model\n",
    "\n",
    "A classical bag-of-words representation like **TF-IDF** ignores word order but can still be\n",
    "viewed (loosely) as a **pseudo language model** if we normalize the TF-IDF vector to obtain a\n",
    "distribution over the vocabulary.\n",
    "\n",
    "In this section we:\n",
    "\n",
    "- Build a TF-IDF representation for Reuters documents\n",
    "- Convert TF-IDF vectors into normalized distributions over terms\n",
    "- Use them to score sentences and compute a pseudo perplexity\n",
    "- Compare this to real n-gram LMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a537af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vocabulary size: 10238\n",
      "TF-IDF pseudo perplexity (validation): 5.766263093773351\n"
     ]
    }
   ],
   "source": [
    "# Import sklearn's TF-IDF implementation and numpy for numerical operations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def join_tokens(sent):\n",
    "    \"\"\"Convert a tokenized sentence back to a string, removing special tokens.\"\"\"\n",
    "    # TF-IDF works with text strings, so we need to join the token list\n",
    "    # Skip <BOS> and <EOS> since they're not real content words\n",
    "    return \" \".join(tok for tok in sent if tok not in {\"<BOS>\", \"<EOS>\"})\n",
    "\n",
    "\n",
    "# Convert all preprocessed sentences back to strings\n",
    "train_docs = [join_tokens(s) for s in train_proc]  # Training documents as strings\n",
    "valid_docs = [join_tokens(s) for s in valid_proc]  # Validation documents as strings\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "# TF-IDF = Term Frequency - Inverse Document Frequency\n",
    "# It weights words by: (how often in this doc) / (how common across all docs)\n",
    "# min_df=5 means ignore words that appear in fewer than 5 documents\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "\n",
    "# Fit the vectorizer on training data and compute TF-IDF scores\n",
    "# This learns the vocabulary and IDF values\n",
    "X_train = vectorizer.fit_transform(train_docs)\n",
    "\n",
    "# Extract vocabulary and create index mapping\n",
    "vocab_list = vectorizer.get_feature_names_out()  # List of all words\n",
    "vocab_index = {w: i for i, w in enumerate(vocab_list)}  # Map word -> position in vector\n",
    "\n",
    "print(\"TF-IDF vocabulary size:\", len(vocab_list))\n",
    "\n",
    "\n",
    "def tfidf_to_prob(vec):\n",
    "    \"\"\"Convert a sparse TF-IDF vector into a normalized probability distribution.\n",
    "    \n",
    "    TF-IDF weights aren't probabilities, but we can normalize them to sum to 1.\n",
    "    This lets us use them as a pseudo-probability distribution over words.\n",
    "    \"\"\"\n",
    "    # Convert sparse matrix to a regular numpy array and flatten to 1D\n",
    "    vec = np.asarray(vec.todense()).flatten()\n",
    "    \n",
    "    # Calculate sum of all TF-IDF weights\n",
    "    total = vec.sum()\n",
    "    \n",
    "    if total == 0.0:\n",
    "        # If all weights are zero, return uniform distribution (all words equally likely)\n",
    "        return np.ones_like(vec) / len(vec)\n",
    "    \n",
    "    # Normalize: divide each weight by total so they sum to 1\n",
    "    return vec / total\n",
    "\n",
    "\n",
    "def sentence_log_prob_tfidf(sent, prob_vec):\n",
    "    \"\"\"Score a sentence by summing log probabilities of its words.\n",
    "    \n",
    "    This treats the TF-IDF distribution as a language model and scores how\n",
    "    likely the sentence is under that distribution.\n",
    "    \"\"\"\n",
    "    log_p = 0.0  # Initialize sum\n",
    "    \n",
    "    # Loop through each word in the sentence\n",
    "    for w in sent:\n",
    "        # Skip special tokens (not real content words)\n",
    "        if w in {\"<BOS>\", \"<EOS>\"}:\n",
    "            continue\n",
    "        \n",
    "        # Find the position of this word in the TF-IDF vocabulary\n",
    "        idx = vocab_index.get(w, None)\n",
    "        \n",
    "        if idx is None:\n",
    "            # Word not in TF-IDF vocabulary, skip it\n",
    "            continue\n",
    "        \n",
    "        # Get probability for this word\n",
    "        p = prob_vec[idx]\n",
    "        \n",
    "        if p > 0.0:\n",
    "            log_p += math.log(p)  # Add log probability\n",
    "    \n",
    "    return log_p\n",
    "\n",
    "\n",
    "def perplexity_tfidf(docs, sents):\n",
    "    \"\"\"Calculate perplexity using TF-IDF as a pseudo language model.\n",
    "    \n",
    "    For each sentence, we:\n",
    "    1. Compute its TF-IDF vector\n",
    "    2. Convert to a probability distribution\n",
    "    3. Score the sentence under that distribution\n",
    "    \"\"\"\n",
    "    total_log_prob = 0.0  # Sum of log probabilities\n",
    "    total_tokens = 0  # Total number of words scored\n",
    "    \n",
    "    # Process each document-sentence pair together\n",
    "    # zip pairs up corresponding elements from two lists\n",
    "    for doc_text, sent in zip(docs, sents):\n",
    "        # Compute TF-IDF vector for this document\n",
    "        vec = vectorizer.transform([doc_text])\n",
    "        \n",
    "        # Convert TF-IDF weights to probabilities\n",
    "        prob_vec = tfidf_to_prob(vec)\n",
    "        \n",
    "        # Score the sentence using these probabilities\n",
    "        log_p = sentence_log_prob_tfidf(sent, prob_vec)\n",
    "        \n",
    "        # Count non-special tokens (actual content words)\n",
    "        n_tokens = sum(1 for w in sent if w not in {\"<BOS>\", \"<EOS>\"})\n",
    "        \n",
    "        # Accumulate totals\n",
    "        total_log_prob += log_p\n",
    "        total_tokens += max(n_tokens, 1)  # Avoid division by zero\n",
    "    \n",
    "    # Calculate average log probability per token\n",
    "    avg_log_p = total_log_prob / total_tokens\n",
    "    \n",
    "    # Convert to perplexity\n",
    "    return math.exp(-avg_log_p)\n",
    "\n",
    "\n",
    "# Evaluate TF-IDF pseudo-LM on validation data\n",
    "print(\"TF-IDF pseudo perplexity (validation):\", perplexity_tfidf(valid_docs, valid_proc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d7758",
   "metadata": {},
   "source": [
    "### Why TF-IDF as a \"Language Model\"?\n",
    "\n",
    "TF-IDF is fundamentally a **bag-of-words** representation that ignores word order. However, we can interpret it as a pseudo language model by:\n",
    "\n",
    "1. Computing TF-IDF weights for each word in a document\n",
    "2. Normalizing these weights to sum to 1, creating a probability distribution\n",
    "3. Using this distribution to score sentences (higher probability for words with high TF-IDF)\n",
    "\n",
    "**Key difference from n-grams:** TF-IDF doesn't model word order or context, so it captures **what** words appear but not **how** they're arranged. This makes it a poor true language model, but useful for comparison to see the importance of modeling sequential structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39babc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF 'generated' text (NOT a real language model!):\n",
      "Note: This looks incoherent because TF-IDF is a bag-of-words model\n",
      "that doesn't capture word order or sequential dependencies.\n",
      "\n",
      "  Example 1: <BOS> trading of steady peak billion <EOS>\n",
      "  Example 2: <BOS> 300 mark to 300 300 <EOS>\n",
      "\n",
      "Compare these 'sentences' to the n-gram outputs above.\n",
      "The n-grams produce locally coherent text because they model word sequences.\n",
      "TF-IDF cannot do this - it only knows which words are topically important.\n"
     ]
    }
   ],
   "source": [
    "def generate_tfidf_pseudo(max_len=20, seed_doc_idx=0):\n",
    "    \"\"\"Generate 'text' from TF-IDF (a hacky demonstration).\n",
    "    \n",
    "    WARNING: This is NOT a real language model! TF-IDF has no sequential structure.\n",
    "    This function artificially samples words from a TF-IDF distribution just to show\n",
    "    what happens when we ignore word order entirely.\n",
    "    \n",
    "    The output will look very random and incoherent because TF-IDF only knows\n",
    "    which words are important, not how words should be ordered or which words\n",
    "    typically follow each other.\n",
    "    \"\"\"\n",
    "    # Get TF-IDF vector for a seed document\n",
    "    seed_doc = train_docs[seed_doc_idx]\n",
    "    vec = vectorizer.transform([seed_doc])\n",
    "    prob_vec = tfidf_to_prob(vec)\n",
    "    \n",
    "    # Sample words randomly according to TF-IDF weights (ignoring all order!)\n",
    "    words = [\"<BOS>\"]\n",
    "    for _ in range(max_len):\n",
    "        # Sample a word based on its TF-IDF weight\n",
    "        idx = np.random.choice(len(prob_vec), p=prob_vec)\n",
    "        word = vocab_list[idx]\n",
    "        words.append(word)\n",
    "        \n",
    "        # Arbitrary stopping condition (just for demonstration)\n",
    "        if len(words) > 5 and random.random() < 0.2:\n",
    "            break\n",
    "    \n",
    "    words.append(\"<EOS>\")\n",
    "    return words\n",
    "\n",
    "\n",
    "# Generate examples to show the lack of sequential coherence\n",
    "print(\"TF-IDF 'generated' text (NOT a real language model!):\")\n",
    "print(\"Note: This looks incoherent because TF-IDF is a bag-of-words model\")\n",
    "print(\"that doesn't capture word order or sequential dependencies.\\n\")\n",
    "\n",
    "# Generate from different seed documents to show variety\n",
    "print(\"  Example 1:\", \" \".join(generate_tfidf_pseudo(max_len=15, seed_doc_idx=0)))\n",
    "print(\"  Example 2:\", \" \".join(generate_tfidf_pseudo(max_len=15, seed_doc_idx=100)))\n",
    "\n",
    "print(\"\\nCompare these 'sentences' to the n-gram outputs above.\")\n",
    "print(\"The n-grams produce locally coherent text because they model word sequences.\")\n",
    "print(\"TF-IDF cannot do this - it only knows which words are topically important.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbb91c",
   "metadata": {},
   "source": [
    "### Reflection 2: Bag-of-Words vs Sequence Models\n",
    "\n",
    "1. In what sense does a TF-IDF vector act like a **language model**? In what sense does it clearly not?\n",
    "2. Compare TF-IDF pseudo perplexity to n-gram perplexities. Why is this comparison imperfect?\n",
    "3. How do these observations match the lecture discussion about:\n",
    "   - Bag-of-words and TF-IDF\n",
    "   - Co-occurrence matrices\n",
    "   - The move toward distributed representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac5c0bf",
   "metadata": {},
   "source": [
    "## 5. Neural Window Language Model with Pretrained Embeddings\n",
    "\n",
    "We now move to a **neural language model** that:\n",
    "\n",
    "- Uses a **fixed window** of context words\n",
    "- Represents each word with a **pretrained embedding**\n",
    "- Predicts the next word using a small feedforward neural network\n",
    "\n",
    "This corresponds to the lecture's transition from sparse n-grams to **dense, distributed representations**.\n",
    "\n",
    "Implementation notes:\n",
    "\n",
    "- For simplicity, we restrict to a small subset of the vocabulary.\n",
    "- We assume access to pretrained embeddings (for example, via `gensim.downloader` or local GloVe files).\n",
    "- Training on the full Reuters corpus can be slow; you may want to use a small sample when running this yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a5bf8",
   "metadata": {},
   "source": [
    "### Understanding Window-Based Neural Language Models\n",
    "\n",
    "Unlike n-grams that use sparse one-hot vectors, neural LMs use **dense word embeddings**:\n",
    "\n",
    "- Each word is represented as a continuous vector (e.g., 100 dimensions)\n",
    "- Similar words have similar vectors, enabling **generalization**\n",
    "- A fixed window of context words is concatenated and fed through a neural network\n",
    "\n",
    "**Architecture overview:**\n",
    "1. Look up embeddings for the last $n$ words (the context window)\n",
    "2. Concatenate these embeddings into a single vector\n",
    "3. Pass through feedforward layers to predict the next word\n",
    "\n",
    "**Key advantage:** If the model has seen \"the cat sat\" and \"the dog sat\", it can generalize to \"the kitten sat\" because cat, dog, and kitten have similar embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00fb735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural LM vocab size: 5000\n",
      "Window dataset size: torch.Size([41851, 3]) torch.Size([41851])\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch - a deep learning library\n",
    "import torch  # Main PyTorch module\n",
    "import torch.nn as nn  # Neural network components\n",
    "import torch.optim as optim  # Optimization algorithms (like gradient descent)\n",
    "\n",
    "# Configuration parameters\n",
    "CONTEXT_SIZE = 3  # How many previous words to look at when predicting the next word\n",
    "MAX_VOCAB_NEURAL = 5000  # Limit vocabulary size for faster training\n",
    "\n",
    "# Build a smaller vocabulary for the neural model (use most frequent words)\n",
    "# Reserve space for special tokens by getting MAX_VOCAB_NEURAL - 3 most common words\n",
    "most_common = freq.most_common(MAX_VOCAB_NEURAL - 3)  # Get top 4997 most frequent words\n",
    "neural_vocab = [w for w, _ in most_common]  # Extract just the words (ignore counts)\n",
    "\n",
    "# Add special tokens to the vocabulary (now we have room for them)\n",
    "for special in [\"<UNK>\", \"<BOS>\", \"<EOS>\"]:\n",
    "    if special not in neural_vocab:\n",
    "        neural_vocab.append(special)  # Add if missing\n",
    "\n",
    "# Create word-to-index and index-to-word dictionaries\n",
    "# Neural networks work with numbers, not words, so we need this mapping\n",
    "neural_stoi = {w: i for i, w in enumerate(neural_vocab)}  # string to index\n",
    "neural_itos = {i: w for w, i in neural_stoi.items()}  # index to string\n",
    "\n",
    "vocab_size_neural = len(neural_vocab)\n",
    "print(\"Neural LM vocab size:\", vocab_size_neural)\n",
    "\n",
    "\n",
    "def sent_to_indices(sent):\n",
    "    \"\"\"Convert a sentence (list of words) to a list of integer indices.\"\"\"\n",
    "    indices = []\n",
    "    for w in sent:\n",
    "        # If word not in our neural vocabulary, use <UNK>\n",
    "        if w not in neural_stoi:\n",
    "            w = \"<UNK>\"\n",
    "        indices.append(neural_stoi[w])  # Look up the index for this word\n",
    "    return indices\n",
    "\n",
    "\n",
    "def build_window_dataset(sentences, context_size=CONTEXT_SIZE, max_sents=5000):\n",
    "    \"\"\"Create training examples: each example is (context_words, target_word).\n",
    "    \n",
    "    For a sentence \"the cat sat on mat\", with context_size=2:\n",
    "    - Example 1: context=[the, cat], target=sat\n",
    "    - Example 2: context=[cat, sat], target=on\n",
    "    - Example 3: context=[sat, on], target=mat\n",
    "    \"\"\"\n",
    "    # Lists to store all training examples\n",
    "    contexts = []  # Each context is a list of context_size word indices\n",
    "    targets = []   # Each target is a single word index to predict\n",
    "    \n",
    "    # Process up to max_sents sentences (to keep dataset manageable)\n",
    "    for sent in sentences[:max_sents]:\n",
    "        # Convert words to indices\n",
    "        word_indices = sent_to_indices(sent)\n",
    "        \n",
    "        # Slide a window across the sentence to create training examples\n",
    "        # Start at position context_size (we need context_size previous words)\n",
    "        for i in range(context_size, len(word_indices)):\n",
    "            # Get the context: previous context_size words\n",
    "            context_window = word_indices[i - context_size : i]\n",
    "            \n",
    "            # Get the target: the word we're trying to predict\n",
    "            next_word = word_indices[i]\n",
    "            \n",
    "            # Add this training example\n",
    "            contexts.append(context_window)\n",
    "            targets.append(next_word)\n",
    "    \n",
    "    # Convert lists to PyTorch tensors\n",
    "    # dtype=torch.long means integers (required for indexing embeddings)\n",
    "    return torch.tensor(contexts, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Build the training dataset\n",
    "X_train_win, y_train_win = build_window_dataset(train_proc, context_size=CONTEXT_SIZE)\n",
    "print(\"Window dataset size:\", X_train_win.shape, y_train_win.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d5684",
   "metadata": {},
   "source": [
    "### The Neural Architecture\n",
    "\n",
    "Our model is a simple feedforward network:\n",
    "\n",
    "```\n",
    "Input: [word_{t-3}, word_{t-2}, word_{t-1}]  (context window)\n",
    "   ↓\n",
    "Embedding Layer: converts each word index to a dense vector\n",
    "   ↓\n",
    "Concatenate: [embed_{t-3} || embed_{t-2} || embed_{t-1}]\n",
    "   ↓\n",
    "Hidden Layer: fully connected with ReLU activation\n",
    "   ↓\n",
    "Output Layer: predicts probability distribution over vocabulary\n",
    "   ↓\n",
    "Output: P(word_t | word_{t-3}, word_{t-2}, word_{t-1})\n",
    "```\n",
    "\n",
    "The embedding layer is where the magic happens: it learns to map words to vectors such that words used in similar contexts end up close together in the embedding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5837ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralWindowLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, context_size=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear1 = nn.Linear(embed_dim * context_size, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, context_size)\n",
    "        embeds = self.embed(x)  \n",
    "        # embeds shape: (batch_size, context_size, embed_dim)\n",
    "        \n",
    "        # Flatten the embeddings for each example\n",
    "        embeds = embeds.view(embeds.size(0), -1)  \n",
    "        # embeds shape: (batch_size, context_size * embed_dim)\n",
    "        \n",
    "        # Pass through hidden layer\n",
    "        h = self.relu(self.linear1(embeds))\n",
    "        # h shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Generate logits for each word in vocabulary\n",
    "        logits = self.linear2(h)\n",
    "        # logits shape: (batch_size, vocab_size)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65906c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training neural language model...\n",
      "Epoch 1/3, Average Loss: 6.0034\n",
      "Epoch 2/3, Average Loss: 4.8322\n",
      "Epoch 3/3, Average Loss: 4.4309\n"
     ]
    }
   ],
   "source": [
    "# Create model instance and training components\n",
    "# Choose device: use GPU if available (faster), otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the neural language model\n",
    "model = NeuralWindowLM(\n",
    "    vocab_size_neural,  # Number of words in vocabulary\n",
    "    embed_dim=100,  # Size of word embedding vectors\n",
    "    context_size=CONTEXT_SIZE,  # Number of context words (3)\n",
    "    hidden_dim=128  # Size of hidden layer\n",
    ").to(device)  # Move model to GPU/CPU\n",
    "\n",
    "# Loss function: measures how wrong the predictions are\n",
    "# CrossEntropyLoss is standard for classification (predicting which word comes next)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer: updates model weights to reduce loss\n",
    "# Adam is a popular optimization algorithm (improved gradient descent)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train_neural_lm(model, X, y, epochs=1, batch_size=256):\n",
    "    \"\"\"Train the neural language model on the provided dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network to train\n",
    "        X: Input data (context windows)\n",
    "        y: Target data (next words)\n",
    "        epochs: Number of times to iterate through the entire dataset\n",
    "        batch_size: Number of examples to process at once\n",
    "    \"\"\"\n",
    "    model.train()  # Put model in training mode\n",
    "    num_examples = X.size(0)  # Total number of training examples\n",
    "    \n",
    "    # Loop through epochs (complete passes through the data)\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle data each epoch to avoid learning order-dependent patterns\n",
    "        shuffle_indices = torch.randperm(num_examples)  # Random permutation of indices\n",
    "        X_shuffled = X[shuffle_indices]  # Shuffle inputs\n",
    "        y_shuffled = y[shuffle_indices]  # Shuffle targets the same way\n",
    "        \n",
    "        epoch_loss = 0.0  # Track total loss for this epoch\n",
    "        num_batches = 0  # Count number of batches\n",
    "        \n",
    "        # Process data in batches (more efficient than one example at a time)\n",
    "        for batch_start in range(0, num_examples, batch_size):\n",
    "            batch_end = batch_start + batch_size\n",
    "            \n",
    "            # Get this batch of data and move to device (GPU/CPU)\n",
    "            X_batch = X_shuffled[batch_start:batch_end].to(device)\n",
    "            y_batch = y_shuffled[batch_start:batch_end].to(device)\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            optimizer.zero_grad()  # Clear gradients from previous step\n",
    "            logits = model(X_batch)  # Get model predictions\n",
    "            loss = criterion(logits, y_batch)  # Calculate loss\n",
    "            \n",
    "            # Backward pass: compute gradients and update weights\n",
    "            loss.backward()  # Compute gradients (how to adjust weights)\n",
    "            optimizer.step()  # Update weights using gradients\n",
    "            \n",
    "            # Track statistics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Print average loss for this epoch\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Train the neural language model\n",
    "# Note: This takes approximately 2-3 minutes on a recent MacBook (CPU)\n",
    "print(\"Training neural language model...\")\n",
    "train_neural_lm(model, X_train_win, y_train_win, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f414f",
   "metadata": {},
   "source": [
    "### 5.1 Perplexity and Text Generation for the Neural LM\n",
    "\n",
    "We now:\n",
    "\n",
    "- Define a perplexity function for the neural LM\n",
    "- Implement a simple left-to-right text generation procedure using the fixed window context\n",
    "\n",
    "Note that this is still a **window-based** model, not an RNN or transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92224e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural LM perplexity (validation): 158.8833076986575\n",
      "\n",
      "Generated sentences (neural LM):\n",
      "  Example 1: <BOS> <BOS> <BOS> income to <UNK> mln <EOS>\n",
      "  Example 2: <BOS> <BOS> <BOS> the <UNK> can , are 80 <UNK> <EOS>\n",
      "\n",
      "Compare the quality of these generated sentences to the n-gram models above.\n",
      "The neural model benefits from learned word embeddings that capture semantic similarity.\n"
     ]
    }
   ],
   "source": [
    "def neural_sent_log_prob(model, sent, context_size=CONTEXT_SIZE):\n",
    "    \"\"\"Calculate log probability of a sentence using the neural language model.\n",
    "    \n",
    "    We slide a context window across the sentence and predict each word\n",
    "    given the previous context_size words.\n",
    "    \"\"\"\n",
    "    model.eval()  # Put model in evaluation mode (turns off dropout, etc.)\n",
    "    \n",
    "    # Convert sentence words to integer indices\n",
    "    idxs = sent_to_indices(sent)\n",
    "    \n",
    "    # If sentence is too short, can't make predictions\n",
    "    if len(idxs) <= context_size:\n",
    "        return 0.0\n",
    "    \n",
    "    log_p = 0.0  # Initialize sum of log probabilities\n",
    "    \n",
    "    # torch.no_grad() disables gradient calculation (faster, uses less memory)\n",
    "    # We don't need gradients since we're not training\n",
    "    with torch.no_grad():\n",
    "        # Slide window across sentence, predicting each word\n",
    "        for i in range(context_size, len(idxs)):\n",
    "            # Get context: the previous context_size words\n",
    "            context = torch.tensor([idxs[i - context_size:i]], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Get model predictions (logits = unnormalized scores)\n",
    "            logits = model(context)\n",
    "            \n",
    "            # Convert logits to probabilities using softmax\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Get probability of the actual next word that appeared\n",
    "            p = probs[0, idxs[i]].item()  # .item() converts to Python number\n",
    "            \n",
    "            if p > 0.0:\n",
    "                log_p += math.log(p)  # Add log probability\n",
    "    \n",
    "    return log_p\n",
    "\n",
    "\n",
    "def neural_perplexity(model, sentences, context_size=CONTEXT_SIZE, max_sents=1000):\n",
    "    \"\"\"Calculate perplexity of the neural LM on a set of sentences.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural language model\n",
    "        sentences: List of tokenized sentences\n",
    "        context_size: Number of context words the model uses\n",
    "        max_sents: Maximum number of sentences to evaluate (for speed)\n",
    "    \"\"\"\n",
    "    total_log_prob = 0.0  # Sum of log probabilities\n",
    "    total_tokens = 0  # Total number of words predicted\n",
    "    \n",
    "    # Evaluate on up to max_sents sentences\n",
    "    for sent in sentences[:max_sents]:\n",
    "        # Get log probability for this sentence\n",
    "        log_p = neural_sent_log_prob(model, sent, context_size=context_size)\n",
    "        \n",
    "        # Count tokens: we predict len(sent) - context_size words\n",
    "        # (can't predict first context_size words, no prior context)\n",
    "        n_tokens = max(len(sent) - context_size, 1)\n",
    "        \n",
    "        # Accumulate totals\n",
    "        total_log_prob += log_p\n",
    "        total_tokens += n_tokens\n",
    "    \n",
    "    # Calculate average log probability per token\n",
    "    avg_log_p = total_log_prob / total_tokens\n",
    "    \n",
    "    # Convert to perplexity\n",
    "    return math.exp(-avg_log_p)\n",
    "\n",
    "\n",
    "def generate_neural(model, max_len=20, context_size=CONTEXT_SIZE, prefix=None):\n",
    "    \"\"\"Generate text using the neural language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained neural language model\n",
    "        max_len: Maximum number of words to generate\n",
    "        context_size: Number of context words the model uses\n",
    "        prefix: Starting words (if None, starts with <BOS> tokens)\n",
    "    \n",
    "    Returns:\n",
    "        List of generated words\n",
    "    \"\"\"\n",
    "    model.eval()  # Put model in evaluation mode\n",
    "    \n",
    "    # If no prefix provided, start with beginning-of-sentence tokens\n",
    "    if prefix is None:\n",
    "        prefix = [\"<BOS>\"] * context_size\n",
    "    \n",
    "    # Start with the prefix words\n",
    "    tokens = prefix[:]\n",
    "    \n",
    "    # No gradient calculation needed for generation\n",
    "    with torch.no_grad():\n",
    "        # Generate up to max_len words\n",
    "        for _ in range(max_len):\n",
    "            # Get the last context_size words as context\n",
    "            context = tokens[-context_size:]\n",
    "            \n",
    "            # Convert words to indices (use <UNK> if word not in vocabulary)\n",
    "            idxs = [neural_stoi.get(w, neural_stoi[\"<UNK>\"]) for w in context]\n",
    "            \n",
    "            # Convert to tensor and move to device\n",
    "            x = torch.tensor([idxs], dtype=torch.long).to(device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            logits = model(x)\n",
    "            \n",
    "            # Convert to probabilities and move to CPU as numpy array\n",
    "            probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "            \n",
    "            # Sample next word according to the probability distribution\n",
    "            next_idx = np.random.choice(len(probs), p=probs)\n",
    "            \n",
    "            # Convert index back to word\n",
    "            next_word = neural_itos[next_idx]\n",
    "            \n",
    "            # Add to generated sequence\n",
    "            tokens.append(next_word)\n",
    "            \n",
    "            # Stop if we generate end-of-sentence\n",
    "            if next_word == \"<EOS>\":\n",
    "                break\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Evaluate and generate text from the neural LM\n",
    "print(\"Neural LM perplexity (validation):\", neural_perplexity(model, valid_proc))\n",
    "\n",
    "# Generate example sentences\n",
    "# Note: Neural LM with learned embeddings can generalize better than n-grams\n",
    "# because similar words have similar representations, enabling the model to\n",
    "# make more informed predictions even for unseen word combinations\n",
    "print(\"\\nGenerated sentences (neural LM):\")\n",
    "print(\"  Example 1:\", \" \".join(generate_neural(model)))\n",
    "print(\"  Example 2:\", \" \".join(generate_neural(model)))\n",
    "\n",
    "print(\"\\nCompare the quality of these generated sentences to the n-gram models above.\")\n",
    "print(\"The neural model benefits from learned word embeddings that capture semantic similarity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9723f117",
   "metadata": {},
   "source": [
    "### Comparing Generated Text Quality Across Models\n",
    "\n",
    "Now that you've seen generation examples from each model, let's compare their characteristics:\n",
    "\n",
    "| Model | Text Quality | Why? |\n",
    "|-------|-------------|------|\n",
    "| **Unigram** | Random, incoherent | Only considers word frequency; no context or word order |\n",
    "| **Bigram** | Local coherence, but brittle | Models word pairs; gets local structure right but can't maintain long-range coherence |\n",
    "| **Interpolated** | Smoother than bigram | Falls back to unigram when bigrams unseen; more robust but still limited to pairs |\n",
    "| **TF-IDF** | Random, topical but incoherent | Bag-of-words: knows *what* words matter but not *how* to order them |\n",
    "| **Neural** | Better generalization | Learned embeddings capture semantic similarity; can generalize to unseen contexts |\n",
    "\n",
    "**Key insight:** As we move from sparse count-based models to dense neural representations, we get better generalization and more coherent text. However, even the neural window model is limited by its fixed context size—this is where RNNs and Transformers (with unlimited context) become important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17ecfa",
   "metadata": {},
   "source": [
    "### Reflection 3: From N-grams to Neural Models (and Beyond)\n",
    "\n",
    "1. In what ways does the neural window LM **reuse information** across contexts more effectively than n-grams?\n",
    "2. How do **embeddings** change the sparsity story compared to one-hot vectors and raw co-occurrence counts?\n",
    "3. Based on the lecture, how would **RNNs** or **attention-based models** (transformers) further extend these ideas?\n",
    "   - What limitations of fixed-window models might they address?\n",
    "   - How do they handle **long-range dependencies** and **variable-length context**?\n",
    "\n",
    "Write a short paragraph connecting:\n",
    "\n",
    "- The MLE and smoothed n-gram models\n",
    "- The TF-IDF pseudo-LM\n",
    "- The neural window LM\n",
    "- The conceptual role of RNNs and attention discussed in the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c2551",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "In this notebook you have:\n",
    "\n",
    "- Built and evaluated **unigram and bigram language models** with MLE.\n",
    "- Applied **UNK smoothing** and **interpolated n-grams** to reduce sparsity.\n",
    "- Interpreted **TF-IDF** as a pseudo language model and compared it to sequence-based LMs.\n",
    "- Implemented a **neural window LM** with embeddings and discussed its evaluation through **perplexity** and **text generation**.\n",
    "- Reflected on how **RNNs** and **attention** fit into the progression from sparse to dense, context-aware models.\n",
    "\n",
    "This mirrors the lecture trajectory:\n",
    "\n",
    "1. Sparsity, preprocessing, n-grams, smoothing\n",
    "2. Neural window models and embeddings as better smoothing\n",
    "3. RNNs and attention as conceptual extensions, appearing here only in reflection prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-preprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
